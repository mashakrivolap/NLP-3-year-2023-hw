{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения в этом домашнем задании я выбрала pymorphy2, natasha, stanza. Два последних используют universal pos tags, у pymorphy2 чуть другие теги, что уравнивается в коде.\n",
    "\n",
    "Для корпуса брала предложения из разных источников, среди которых были:\n",
    "\n",
    "- статьи о моде \n",
    "- отрывки из стихотворений с окказионализмами\n",
    "- предложения с информацией о различных учреждениях, записанных аббревиатурами или сокращениями\n",
    "\n",
    "В первом случае интерес представляли заимствованные слова-неологизмы, которых ещё, возможно, нет в словарях, а также слова, написанные через дефис, которые теггеры могли бы разделить, а могли бы и нет (бьюти-тренды, ультра-клеш и т.п.)\n",
    "Во втором были неоднозначными сами авторские окказионализмы, также отсутствующие в словарях. И в последнем случае трудности для теггинга должны были представить сами аббревиатуры и разнообразные сокращения.\n",
    "Также в отобранных предложенияхх встречались имена и фамилии, в том числе иностранные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZgmOKuYDi590"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "from natasha import Doc, MorphVocab, NewsEmbedding, NewsMorphTagger, Segmenter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_bFo7b0tiYv7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 22:50:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228582ec11eb4017af7fc3f4be558fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055d504465544835a88d288e31d40177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/tokenize/syntagrus.pt:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4887b89bbea54dd2af349278c51e1980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/pos/syntagrus_charlm.pt:   0%| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156d797c25d644d7a2a61d9f9d05aa26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/lemma/syntagrus_nocharlm.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d90162329134edea80fda3cc5262b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/depparse/syntagrus_charlm.pt:  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87737304267f49cf919ce33d6097a6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/ner/wikiner.pt:   0%|          …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033e18505c194436b057202e0d23007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/forward_charlm/newswiki.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7845b9ab92c48d69e1a46a4d91a4cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/pretrain/conll17.pt:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65fce5e135d472db680631a7bcfd36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/pretrain/fasttextwiki.pt:   0%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3639895adcb8484b88c887864904f877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.6.0/models/backward_charlm/newswiki.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 22:58:17 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "| depparse  | syntagrus_charlm   |\n",
      "| ner       | wikiner            |\n",
      "==================================\n",
      "\n",
      "2023-10-07 22:58:17 INFO: Using device: cpu\n",
      "2023-10-07 22:58:17 INFO: Loading: tokenize\n",
      "2023-10-07 22:58:18 INFO: Loading: pos\n",
      "2023-10-07 22:58:19 INFO: Loading: lemma\n",
      "2023-10-07 22:58:19 INFO: Loading: depparse\n",
      "2023-10-07 22:58:20 INFO: Loading: ner\n",
      "2023-10-07 22:58:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# загрузка моделек\n",
    "stanza_pipeline = stanza.Pipeline(\"ru\")\n",
    "segmenter = Segmenter()\n",
    "natasha_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KEWJedKyimBZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    }
   ],
   "source": [
    "# чтение корпуса из файла и заодно проверка количества слов\n",
    "with open(\"corpuss.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    print(len(text.split()))\n",
    "    text = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5oPODesjiow-"
   },
   "outputs": [],
   "source": [
    "#Stanza\n",
    "stanza = []\n",
    "for line in text:\n",
    "    stanza_doc = stanza_pipeline(line)\n",
    "    stanza_dict = {}\n",
    "    for sent in stanza_doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos != \"PUNCT\":\n",
    "                stanza_dict[word.text] = word.upos\n",
    "    for key, value in stanza_dict.items():\n",
    "        if value == \"SCONJ\" or value == \"CCONJ\":\n",
    "            stanza_dict[key] = \"CONJ\"\n",
    "        elif value == \"PROPN\":\n",
    "            stanza_dict[key] = \"NOUN\"\n",
    "        elif value == \"AUX\":\n",
    "            stanza_dict[key] = \"VERB\"\n",
    "    stanza.append(stanza_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M8t0wncHixle"
   },
   "outputs": [],
   "source": [
    "#Natasha\n",
    "natasha = []\n",
    "for line in text:\n",
    "    natasha_doc = Doc(line)\n",
    "    natasha_doc.segment(segmenter)\n",
    "    natasha_doc.tag_morph(morph_tagger)\n",
    "    natasha_dict = {}\n",
    "    for token in natasha_doc.tokens:\n",
    "        if token.pos != \"PUNCT\":\n",
    "            natasha_dict[token.text] = token.pos\n",
    "    for key, value in natasha_dict.items():\n",
    "        if value == \"SCONJ\" or value == \"CCONJ\":\n",
    "            natasha_dict[key] = \"CONJ\"\n",
    "        elif value == \"PROPN\":\n",
    "            natasha_dict[key] = \"NOUN\"\n",
    "        elif value == \"AUX\":\n",
    "            natasha_dict[key] = \"VERB\"\n",
    "    natasha.append(natasha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pbDAULcu6NM2"
   },
   "outputs": [],
   "source": [
    "#pymorphy2\n",
    "py = []\n",
    "for line in text:\n",
    "    tokens = word_tokenize(line)\n",
    "    py_dict = {}\n",
    "    for token in tokens:\n",
    "        analysis = morph.parse(token)[0]\n",
    "        if str(analysis.tag).split(\",\")[0] != \"PNCT\":\n",
    "            py_dict[token] = str(analysis.tag).split(\",\")[0].split()[0]\n",
    "    for key, value in py_dict.items():\n",
    "        if value == \"ADVB\" or value == \"PRED\":\n",
    "            py_dict[key] = \"ADV\"\n",
    "        elif value == \"ADJF\":\n",
    "            py_dict[key] = \"ADJ\"\n",
    "        elif value == \"INFN\":\n",
    "            py_dict[key] = \"VERB\"\n",
    "        elif value == \"COMP\":\n",
    "            py_dict[key] = \"ADJ\"\n",
    "        elif value == \"NPRO\":\n",
    "            py_dict[key] = \"PRON\"\n",
    "        elif value == \"PREP\":\n",
    "            py_dict[key] = \"ADP\"\n",
    "        elif value == \"PRTF\" or value == \"PRTS\":\n",
    "            py_dict[key] = \"VERB\"\n",
    "        elif value == \"PRCL\":\n",
    "            py_dict[key] = \"PART\"\n",
    "    py.append(py_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ud2vNI5wi0ma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanza:\n",
      "Специально: ADV\n",
      "для: ADP\n",
      "вас: PRON\n",
      "мы: PRON\n",
      "собрали: VERB\n",
      "большой: ADJ\n",
      "шопинг: NOUN\n",
      "гид: NOUN\n",
      "по: ADP\n",
      "осенним: ADJ\n",
      "трендам: NOUN\n",
      "чтобы: CONJ\n",
      "подготовка: NOUN\n",
      "к: ADP\n",
      "новому: ADJ\n",
      "сезону: NOUN\n",
      "не: PART\n",
      "вызывала: VERB\n",
      "негативных: ADJ\n",
      "эмоций: NOUN\n",
      "и: CONJ\n",
      "проходила: VERB\n",
      "легко: ADV\n",
      "\n",
      "\n",
      "Среди: ADP\n",
      "главных: ADJ\n",
      "трендов: NOUN\n",
      "оказались: VERB\n",
      "традиционно: ADV\n",
      "мужские: ADJ\n",
      "туфли: NOUN\n",
      "оксфорды: NOUN\n",
      "броги: NOUN\n",
      "и: CONJ\n",
      "монки: NOUN\n",
      "\n",
      "\n",
      "Платья: NOUN\n",
      "с: ADP\n",
      "бахромой: NOUN\n",
      "и: CONJ\n",
      "драпировками: NOUN\n",
      "металлик: NOUN\n",
      "брюки: NOUN\n",
      "карго: NOUN\n",
      "все: DET\n",
      "это: PRON\n",
      "стабильно: ADV\n",
      "не: PART\n",
      "покидает: VERB\n",
      "модную: ADJ\n",
      "арену: NOUN\n",
      "\n",
      "\n",
      "Причем: CONJ\n",
      "речь: NOUN\n",
      "идет: VERB\n",
      "не: PART\n",
      "о: ADP\n",
      "легком: ADJ\n",
      "клеше: NOUN\n",
      "который: PRON\n",
      "мы: PRON\n",
      "привыкли: VERB\n",
      "видеть: VERB\n",
      "на: ADP\n",
      "инфлюэнсерах: NOUN\n",
      "в: ADP\n",
      "последние: ADJ\n",
      "годы: NOUN\n",
      "а: CONJ\n",
      "настоящий: ADJ\n",
      "ультра: NOUN\n",
      "клеш: NOUN\n",
      "с: ADP\n",
      "расширенными: VERB\n",
      "книзу: NOUN\n",
      "штанинами: NOUN\n",
      "до: ADP\n",
      "такой: DET\n",
      "степени: NOUN\n",
      "что: CONJ\n",
      "них: PRON\n",
      "можно: ADV\n",
      "запутаться: VERB\n",
      "\n",
      "\n",
      "Она: PRON\n",
      "не: PART\n",
      "расстается: VERB\n",
      "с: ADP\n",
      "ультра: NOUN\n",
      "короткими: ADJ\n",
      "юбками: NOUN\n",
      "ни: PART\n",
      "на: ADP\n",
      "один: NUM\n",
      "публичный: ADJ\n",
      "выход: NOUN\n",
      "и: CONJ\n",
      "безупречно: ADV\n",
      "стилизует: VERB\n",
      "их: PRON\n",
      "по: ADP\n",
      "лучшим: ADJ\n",
      "традициям: NOUN\n",
      "прошлой: ADJ\n",
      "эпохи: NOUN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Stanza:\")\n",
    "for d in stanza[:5]:\n",
    "    for word, pos in d.items():\n",
    "        print(f\"{word}: {pos}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jnTzfYXpgbeq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natasha:\n",
      "Специально: ADV\n",
      "для: ADP\n",
      "вас: PRON\n",
      "мы: PRON\n",
      "собрали: VERB\n",
      "большой: ADJ\n",
      "шопинг-гид: NOUN\n",
      "по: ADP\n",
      "осенним: ADJ\n",
      "трендам: NOUN\n",
      "чтобы: CONJ\n",
      "подготовка: NOUN\n",
      "к: ADP\n",
      "новому: ADJ\n",
      "сезону: NOUN\n",
      "не: PART\n",
      "вызывала: VERB\n",
      "негативных: ADJ\n",
      "эмоций: NOUN\n",
      "и: CONJ\n",
      "проходила: VERB\n",
      "легко: ADV\n",
      "\n",
      "\n",
      "Среди: ADP\n",
      "главных: ADJ\n",
      "трендов: NOUN\n",
      "оказались: VERB\n",
      "традиционно: ADV\n",
      "мужские: ADJ\n",
      "туфли: NOUN\n",
      "оксфорды: NOUN\n",
      "броги: NOUN\n",
      "и: CONJ\n",
      "монки: NOUN\n",
      "\n",
      "\n",
      "Платья: NOUN\n",
      "с: ADP\n",
      "бахромой: NOUN\n",
      "и: CONJ\n",
      "драпировками: NOUN\n",
      "металлик: NOUN\n",
      "брюки: NOUN\n",
      "карго: NOUN\n",
      "все: DET\n",
      "это: PRON\n",
      "стабильно: ADV\n",
      "не: PART\n",
      "покидает: VERB\n",
      "модную: ADJ\n",
      "арену: NOUN\n",
      "\n",
      "\n",
      "Причем: CONJ\n",
      "речь: NOUN\n",
      "идет: VERB\n",
      "не: PART\n",
      "о: ADP\n",
      "легком: ADJ\n",
      "клеше: NOUN\n",
      "который: PRON\n",
      "мы: PRON\n",
      "привыкли: VERB\n",
      "видеть: VERB\n",
      "на: ADP\n",
      "инфлюэнсерах: NOUN\n",
      "в: ADP\n",
      "последние: ADJ\n",
      "годы: NOUN\n",
      "а: CONJ\n",
      "настоящий: ADJ\n",
      "ультра-клеш: NOUN\n",
      "с: ADP\n",
      "расширенными: VERB\n",
      "книзу: NOUN\n",
      "штанинами: NOUN\n",
      "до: ADP\n",
      "такой: DET\n",
      "степени: NOUN\n",
      "что: CONJ\n",
      "них: PRON\n",
      "можно: ADV\n",
      "запутаться: VERB\n",
      "\n",
      "\n",
      "Она: PRON\n",
      "не: PART\n",
      "расстается: VERB\n",
      "с: ADP\n",
      "ультра-короткими: ADJ\n",
      "юбками: NOUN\n",
      "ни: PART\n",
      "на: ADP\n",
      "один: NUM\n",
      "публичный: ADJ\n",
      "выход: NOUN\n",
      "и: CONJ\n",
      "безупречно: ADV\n",
      "стилизует: VERB\n",
      "их: PRON\n",
      "по: ADP\n",
      "лучшим: ADJ\n",
      "традициям: NOUN\n",
      "прошлой: ADJ\n",
      "эпохи: NOUN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Natasha:\")\n",
    "for d in natasha[:5]:\n",
    "    for word, pos in d.items():\n",
    "        print(f\"{word}: {pos}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "K9veJEdLgbns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy:\n",
      "Специально: ADV\n",
      "для: ADP\n",
      "вас: PRON\n",
      "мы: PRON\n",
      "собрали: VERB\n",
      "большой: ADJ\n",
      "шопинг-гид: NOUN\n",
      "по: ADP\n",
      "осенним: ADJ\n",
      "трендам: NOUN\n",
      "чтобы: CONJ\n",
      "подготовка: NOUN\n",
      "к: ADP\n",
      "новому: ADJ\n",
      "сезону: NOUN\n",
      "не: PART\n",
      "вызывала: VERB\n",
      "негативных: ADJ\n",
      "эмоций: NOUN\n",
      "и: CONJ\n",
      "проходила: VERB\n",
      "легко: ADV\n",
      "\n",
      "\n",
      "Среди: ADP\n",
      "главных: ADJ\n",
      "трендов: NOUN\n",
      "оказались: VERB\n",
      "традиционно: ADV\n",
      "мужские: ADJ\n",
      "туфли: NOUN\n",
      "оксфорды: NOUN\n",
      "броги: ADJS\n",
      "и: CONJ\n",
      "монки: NOUN\n",
      "\n",
      "\n",
      "Платья: NOUN\n",
      "с: ADP\n",
      "бахромой: NOUN\n",
      "и: CONJ\n",
      "драпировками: NOUN\n",
      "металлик: NOUN\n",
      "брюки: NOUN\n",
      "карго: NOUN\n",
      "все: PART\n",
      "это: PART\n",
      "стабильно: ADV\n",
      "не: PART\n",
      "покидает: VERB\n",
      "модную: ADJ\n",
      "арену: NOUN\n",
      "\n",
      "\n",
      "Причем: CONJ\n",
      "речь: NOUN\n",
      "идет: VERB\n",
      "не: PART\n",
      "о: ADP\n",
      "легком: ADJ\n",
      "клеше: NOUN\n",
      "который: ADJ\n",
      "мы: PRON\n",
      "привыкли: VERB\n",
      "видеть: VERB\n",
      "на: ADP\n",
      "инфлюэнсерах: NOUN\n",
      "в: ADP\n",
      "последние: ADJ\n",
      "годы: NOUN\n",
      "а: CONJ\n",
      "настоящий: ADJ\n",
      "ультра-клеш: NOUN\n",
      "с: ADP\n",
      "расширенными: VERB\n",
      "книзу: ADV\n",
      "штанинами: NOUN\n",
      "до: ADP\n",
      "такой: ADJ\n",
      "степени: NOUN\n",
      "что: CONJ\n",
      "них: PRON\n",
      "можно: ADV\n",
      "запутаться: VERB\n",
      "\n",
      "\n",
      "Она: PRON\n",
      "не: PART\n",
      "расстается: VERB\n",
      "с: ADP\n",
      "ультра-короткими: ADJ\n",
      "юбками: NOUN\n",
      "ни: PART\n",
      "на: ADP\n",
      "один: ADJ\n",
      "публичный: ADJ\n",
      "выход: NOUN\n",
      "и: CONJ\n",
      "безупречно: ADV\n",
      "стилизует: VERB\n",
      "их: PRON\n",
      "по: ADP\n",
      "лучшим: ADJ\n",
      "традициям: NOUN\n",
      "прошлой: ADJ\n",
      "эпохи: NOUN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Pymorphy:\")\n",
    "for d in py[:5]:\n",
    "    for word, pos in d.items():\n",
    "        print(f\"{word}: {pos}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vILnGeilBwyJ"
   },
   "outputs": [],
   "source": [
    "#достаем нашу разметку\n",
    "answers = []\n",
    "with open(\"answers.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    sents = file.read().split(\"\\n\\n\\n\")\n",
    "    for s in sents:\n",
    "        d = {}\n",
    "        pairs = s.split(\"\\n\")\n",
    "        for pair in pairs:\n",
    "            d[pair.split()[0]] = pair.split()[1]\n",
    "        answers.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bB6O8tQHv6eZ"
   },
   "outputs": [],
   "source": [
    "#так будем считать, что лучше\n",
    "def accuracy(ans, pred):\n",
    "    sum = 0\n",
    "    score = 0\n",
    "    for i in range(0, len(ans)):\n",
    "        for word in ans[i]:\n",
    "            sum += 1\n",
    "            if word in pred[i]:\n",
    "                if ans[i][word] == pred[i][word]:\n",
    "                    score += 1\n",
    "    return score/sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGdSyT6tIHqs",
    "outputId": "28a6d99e-99d5-48b9-f4b2-5c6244a65ab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9155555555555556"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(answers, stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1abLA4BG97p",
    "outputId": "0e44e02f-10fd-4ec6-f4aa-51bb01b79538"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8933333333333333"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(answers, natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWQykHMNIJaX",
    "outputId": "27635465-6323-42e1-e49a-b8db032ef197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8844444444444445"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(answers, py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всех справилась stanza!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gramm-ы, которые могли бы помочь в прошлой домашке (на большее меня не хватило sorry i'm weak like that):\n",
    "1. самая база, про которую я писала и в прошлой домашке: *не* + глагол / *не* + прилагательное, меняющее окраску глагола/прилагательного на противоположное значение (не стала разделять на два пункта глагол и прилагательное, потому что суть в целом одна).\n",
    "2. *до* + существительное + *далеко*. Часто могут использоваться фразы типа *до идеала далеко*, *до уровня далеко*, и в контексте такой фразы само по себе положительное слово (напр. *идеал*) уже не несет в себе положительной окраски, а выступает средством контраста\n",
    "3. *якобы* + прилагательное. Тут похожая ситуация с *не*: употребление такого сочетания может превратить какое-либо положительное по окраске прилагательное в средство невыгодного сравнения: *якобы престижный*, *якобы интересный*, то есть общая эмоциональная окраска биграммы будет уже негативной."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
